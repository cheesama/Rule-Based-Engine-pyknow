{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from collections import defaultdict\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample_text.txt', 'r') as f:\n",
    "    text=f.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In January, Google launched a new service called Cloud AutoML, which can automate some tricky aspects of designing machine-learning software.',\n",
       " \"While working on this project, the company's researchers sometimes needed to run as many as 800 graphics chips in unison to train their powerful algorithms.Unlike humans, who can recognize coffee cups from seeing one or two examples, AI networks based on simulated neurons need to see tens of thousands of examples in order to identify an object.\",\n",
       " 'Imagine trying to learn to recognize every item in your environment that way, and you begin to understand why AI software requires so much computing power.If researchers could design neural networks that could be trained to do certain tasks using only a handful of examples, it would \"upend the whole paradigm\" Charles Bergan, vice president of engineering at Qualcomm, told the crowd at MIT Technology Review\\'s EmTech China conference earlier this week.If neural networks were to become capable of \"one-shot learning\" Bergan said, the cumbersome process of feeding reams of data into algorithms to train them would be rendered obsolete.',\n",
       " \"This could have serious consequences for the hardware industry, as both existing tech giants and startups are currently focused on developing more powerful processors designed to run today's data-intensive AI algorithms.It would also mean vastly more efficient machine learning.\",\n",
       " 'While neural networks that can be trained using small data sets are not a reality yet, research is already being done on making algorithms smaller without losing accuracy, Bill Dally, chief scientist at Nvidia, said at the conference.Nvidia researchers use a process called network pruning to to make a neural network smaller and more efficient to run by removing the neurons that do no contribute directly to output.',\n",
       " '\"There are ways of training that can reduce the complexity of training by huge amounts\" Dally said.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_words = sent_tokenize(text)\n",
    "sent_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "Now we are spliting the text to words. In the first example we are going to use simple tokinizer. It splits the words. It might look trivial, like using .split('.') function and for many sentences it is. However spliting the words is also a challenge, especially when you face things like concatenations like ```we are``` and ```we're```. NLTK is going ahead and save your time with this seemingly simple, yet complex operations.\n",
    "\n",
    "In the next step there is implemented tokenizer which will also exclude the puntuations from the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'january',\n",
       " ',',\n",
       " 'google',\n",
       " 'launched',\n",
       " 'a',\n",
       " 'new',\n",
       " 'service',\n",
       " 'called',\n",
       " 'cloud',\n",
       " 'automl',\n",
       " ',',\n",
       " 'which',\n",
       " 'can',\n",
       " 'automate',\n",
       " 'some',\n",
       " 'tricky',\n",
       " 'aspects',\n",
       " 'of',\n",
       " 'designing',\n",
       " 'machine-learning',\n",
       " 'software',\n",
       " '.',\n",
       " 'while',\n",
       " 'working',\n",
       " 'on',\n",
       " 'this',\n",
       " 'project',\n",
       " ',',\n",
       " 'the',\n",
       " 'company',\n",
       " \"'s\",\n",
       " 'researchers',\n",
       " 'sometimes',\n",
       " 'needed',\n",
       " 'to',\n",
       " 'run',\n",
       " 'as',\n",
       " 'many',\n",
       " 'as',\n",
       " '800',\n",
       " 'graphics',\n",
       " 'chips',\n",
       " 'in',\n",
       " 'unison',\n",
       " 'to',\n",
       " 'train',\n",
       " 'their',\n",
       " 'powerful',\n",
       " 'algorithms.unlike',\n",
       " 'humans',\n",
       " ',',\n",
       " 'who',\n",
       " 'can',\n",
       " 'recognize',\n",
       " 'coffee',\n",
       " 'cups',\n",
       " 'from',\n",
       " 'seeing',\n",
       " 'one',\n",
       " 'or',\n",
       " 'two',\n",
       " 'examples',\n",
       " ',',\n",
       " 'ai',\n",
       " 'networks',\n",
       " 'based',\n",
       " 'on',\n",
       " 'simulated',\n",
       " 'neurons',\n",
       " 'need',\n",
       " 'to',\n",
       " 'see',\n",
       " 'tens',\n",
       " 'of',\n",
       " 'thousands',\n",
       " 'of',\n",
       " 'examples',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'identify',\n",
       " 'an',\n",
       " 'object',\n",
       " '.',\n",
       " 'imagine',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'to',\n",
       " 'recognize',\n",
       " 'every',\n",
       " 'item',\n",
       " 'in',\n",
       " 'your',\n",
       " 'environment',\n",
       " 'that',\n",
       " 'way',\n",
       " ',',\n",
       " 'and',\n",
       " 'you',\n",
       " 'begin',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'why',\n",
       " 'ai',\n",
       " 'software',\n",
       " 'requires',\n",
       " 'so',\n",
       " 'much',\n",
       " 'computing',\n",
       " 'power.if',\n",
       " 'researchers',\n",
       " 'could',\n",
       " 'design',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'that',\n",
       " 'could',\n",
       " 'be',\n",
       " 'trained',\n",
       " 'to',\n",
       " 'do',\n",
       " 'certain',\n",
       " 'tasks',\n",
       " 'using',\n",
       " 'only',\n",
       " 'a',\n",
       " 'handful',\n",
       " 'of',\n",
       " 'examples',\n",
       " ',',\n",
       " 'it',\n",
       " 'would',\n",
       " '``',\n",
       " 'upend',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'paradigm',\n",
       " \"''\",\n",
       " 'charles',\n",
       " 'bergan',\n",
       " ',',\n",
       " 'vice',\n",
       " 'president',\n",
       " 'of',\n",
       " 'engineering',\n",
       " 'at',\n",
       " 'qualcomm',\n",
       " ',',\n",
       " 'told',\n",
       " 'the',\n",
       " 'crowd',\n",
       " 'at',\n",
       " 'mit',\n",
       " 'technology',\n",
       " 'review',\n",
       " \"'s\",\n",
       " 'emtech',\n",
       " 'china',\n",
       " 'conference',\n",
       " 'earlier',\n",
       " 'this',\n",
       " 'week.if',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'were',\n",
       " 'to',\n",
       " 'become',\n",
       " 'capable',\n",
       " 'of',\n",
       " '``',\n",
       " 'one-shot',\n",
       " 'learning',\n",
       " \"''\",\n",
       " 'bergan',\n",
       " 'said',\n",
       " ',',\n",
       " 'the',\n",
       " 'cumbersome',\n",
       " 'process',\n",
       " 'of',\n",
       " 'feeding',\n",
       " 'reams',\n",
       " 'of',\n",
       " 'data',\n",
       " 'into',\n",
       " 'algorithms',\n",
       " 'to',\n",
       " 'train',\n",
       " 'them',\n",
       " 'would',\n",
       " 'be',\n",
       " 'rendered',\n",
       " 'obsolete',\n",
       " '.',\n",
       " 'this',\n",
       " 'could',\n",
       " 'have',\n",
       " 'serious',\n",
       " 'consequences',\n",
       " 'for',\n",
       " 'the',\n",
       " 'hardware',\n",
       " 'industry',\n",
       " ',',\n",
       " 'as',\n",
       " 'both',\n",
       " 'existing',\n",
       " 'tech',\n",
       " 'giants',\n",
       " 'and',\n",
       " 'startups',\n",
       " 'are',\n",
       " 'currently',\n",
       " 'focused',\n",
       " 'on',\n",
       " 'developing',\n",
       " 'more',\n",
       " 'powerful',\n",
       " 'processors',\n",
       " 'designed',\n",
       " 'to',\n",
       " 'run',\n",
       " 'today',\n",
       " \"'s\",\n",
       " 'data-intensive',\n",
       " 'ai',\n",
       " 'algorithms.it',\n",
       " 'would',\n",
       " 'also',\n",
       " 'mean',\n",
       " 'vastly',\n",
       " 'more',\n",
       " 'efficient',\n",
       " 'machine',\n",
       " 'learning',\n",
       " '.',\n",
       " 'while',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'trained',\n",
       " 'using',\n",
       " 'small',\n",
       " 'data',\n",
       " 'sets',\n",
       " 'are',\n",
       " 'not',\n",
       " 'a',\n",
       " 'reality',\n",
       " 'yet',\n",
       " ',',\n",
       " 'research',\n",
       " 'is',\n",
       " 'already',\n",
       " 'being',\n",
       " 'done',\n",
       " 'on',\n",
       " 'making',\n",
       " 'algorithms',\n",
       " 'smaller',\n",
       " 'without',\n",
       " 'losing',\n",
       " 'accuracy',\n",
       " ',',\n",
       " 'bill',\n",
       " 'dally',\n",
       " ',',\n",
       " 'chief',\n",
       " 'scientist',\n",
       " 'at',\n",
       " 'nvidia',\n",
       " ',',\n",
       " 'said',\n",
       " 'at',\n",
       " 'the',\n",
       " 'conference.nvidia',\n",
       " 'researchers',\n",
       " 'use',\n",
       " 'a',\n",
       " 'process',\n",
       " 'called',\n",
       " 'network',\n",
       " 'pruning',\n",
       " 'to',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'smaller',\n",
       " 'and',\n",
       " 'more',\n",
       " 'efficient',\n",
       " 'to',\n",
       " 'run',\n",
       " 'by',\n",
       " 'removing',\n",
       " 'the',\n",
       " 'neurons',\n",
       " 'that',\n",
       " 'do',\n",
       " 'no',\n",
       " 'contribute',\n",
       " 'directly',\n",
       " 'to',\n",
       " 'output',\n",
       " '.',\n",
       " '``',\n",
       " 'there',\n",
       " 'are',\n",
       " 'ways',\n",
       " 'of',\n",
       " 'training',\n",
       " 'that',\n",
       " 'can',\n",
       " 'reduce',\n",
       " 'the',\n",
       " 'complexity',\n",
       " 'of',\n",
       " 'training',\n",
       " 'by',\n",
       " 'huge',\n",
       " 'amounts',\n",
       " \"''\",\n",
       " 'dally',\n",
       " 'said',\n",
       " '.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "words = word_tokenize(text)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'january',\n",
       " 'google',\n",
       " 'launched',\n",
       " 'a',\n",
       " 'new',\n",
       " 'service',\n",
       " 'called',\n",
       " 'cloud',\n",
       " 'automl',\n",
       " 'which',\n",
       " 'can',\n",
       " 'automate',\n",
       " 'some',\n",
       " 'tricky',\n",
       " 'aspects',\n",
       " 'of',\n",
       " 'designing',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'software',\n",
       " 'while',\n",
       " 'working',\n",
       " 'on',\n",
       " 'this',\n",
       " 'project',\n",
       " 'the',\n",
       " 'company',\n",
       " 's',\n",
       " 'researchers',\n",
       " 'sometimes',\n",
       " 'needed',\n",
       " 'to',\n",
       " 'run',\n",
       " 'as',\n",
       " 'many',\n",
       " 'as',\n",
       " '800',\n",
       " 'graphics',\n",
       " 'chips',\n",
       " 'in',\n",
       " 'unison',\n",
       " 'to',\n",
       " 'train',\n",
       " 'their',\n",
       " 'powerful',\n",
       " 'algorithms',\n",
       " 'unlike',\n",
       " 'humans',\n",
       " 'who',\n",
       " 'can',\n",
       " 'recognize',\n",
       " 'coffee',\n",
       " 'cups',\n",
       " 'from',\n",
       " 'seeing',\n",
       " 'one',\n",
       " 'or',\n",
       " 'two',\n",
       " 'examples',\n",
       " 'ai',\n",
       " 'networks',\n",
       " 'based',\n",
       " 'on',\n",
       " 'simulated',\n",
       " 'neurons',\n",
       " 'need',\n",
       " 'to',\n",
       " 'see',\n",
       " 'tens',\n",
       " 'of',\n",
       " 'thousands',\n",
       " 'of',\n",
       " 'examples',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'identify',\n",
       " 'an',\n",
       " 'object',\n",
       " 'imagine',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'to',\n",
       " 'recognize',\n",
       " 'every',\n",
       " 'item',\n",
       " 'in',\n",
       " 'your',\n",
       " 'environment',\n",
       " 'that',\n",
       " 'way',\n",
       " 'and',\n",
       " 'you',\n",
       " 'begin',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'why',\n",
       " 'ai',\n",
       " 'software',\n",
       " 'requires',\n",
       " 'so',\n",
       " 'much',\n",
       " 'computing',\n",
       " 'power',\n",
       " 'if',\n",
       " 'researchers',\n",
       " 'could',\n",
       " 'design',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'that',\n",
       " 'could',\n",
       " 'be',\n",
       " 'trained',\n",
       " 'to',\n",
       " 'do',\n",
       " 'certain',\n",
       " 'tasks',\n",
       " 'using',\n",
       " 'only',\n",
       " 'a',\n",
       " 'handful',\n",
       " 'of',\n",
       " 'examples',\n",
       " 'it',\n",
       " 'would',\n",
       " 'upend',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'paradigm',\n",
       " 'charles',\n",
       " 'bergan',\n",
       " 'vice',\n",
       " 'president',\n",
       " 'of',\n",
       " 'engineering',\n",
       " 'at',\n",
       " 'qualcomm',\n",
       " 'told',\n",
       " 'the',\n",
       " 'crowd',\n",
       " 'at',\n",
       " 'mit',\n",
       " 'technology',\n",
       " 'review',\n",
       " 's',\n",
       " 'emtech',\n",
       " 'china',\n",
       " 'conference',\n",
       " 'earlier',\n",
       " 'this',\n",
       " 'week',\n",
       " 'if',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'were',\n",
       " 'to',\n",
       " 'become',\n",
       " 'capable',\n",
       " 'of',\n",
       " 'one',\n",
       " 'shot',\n",
       " 'learning',\n",
       " 'bergan',\n",
       " 'said',\n",
       " 'the',\n",
       " 'cumbersome',\n",
       " 'process',\n",
       " 'of',\n",
       " 'feeding',\n",
       " 'reams',\n",
       " 'of',\n",
       " 'data',\n",
       " 'into',\n",
       " 'algorithms',\n",
       " 'to',\n",
       " 'train',\n",
       " 'them',\n",
       " 'would',\n",
       " 'be',\n",
       " 'rendered',\n",
       " 'obsolete',\n",
       " 'this',\n",
       " 'could',\n",
       " 'have',\n",
       " 'serious',\n",
       " 'consequences',\n",
       " 'for',\n",
       " 'the',\n",
       " 'hardware',\n",
       " 'industry',\n",
       " 'as',\n",
       " 'both',\n",
       " 'existing',\n",
       " 'tech',\n",
       " 'giants',\n",
       " 'and',\n",
       " 'startups',\n",
       " 'are',\n",
       " 'currently',\n",
       " 'focused',\n",
       " 'on',\n",
       " 'developing',\n",
       " 'more',\n",
       " 'powerful',\n",
       " 'processors',\n",
       " 'designed',\n",
       " 'to',\n",
       " 'run',\n",
       " 'today',\n",
       " 's',\n",
       " 'data',\n",
       " 'intensive',\n",
       " 'ai',\n",
       " 'algorithms',\n",
       " 'it',\n",
       " 'would',\n",
       " 'also',\n",
       " 'mean',\n",
       " 'vastly',\n",
       " 'more',\n",
       " 'efficient',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'while',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'trained',\n",
       " 'using',\n",
       " 'small',\n",
       " 'data',\n",
       " 'sets',\n",
       " 'are',\n",
       " 'not',\n",
       " 'a',\n",
       " 'reality',\n",
       " 'yet',\n",
       " 'research',\n",
       " 'is',\n",
       " 'already',\n",
       " 'being',\n",
       " 'done',\n",
       " 'on',\n",
       " 'making',\n",
       " 'algorithms',\n",
       " 'smaller',\n",
       " 'without',\n",
       " 'losing',\n",
       " 'accuracy',\n",
       " 'bill',\n",
       " 'dally',\n",
       " 'chief',\n",
       " 'scientist',\n",
       " 'at',\n",
       " 'nvidia',\n",
       " 'said',\n",
       " 'at',\n",
       " 'the',\n",
       " 'conference',\n",
       " 'nvidia',\n",
       " 'researchers',\n",
       " 'use',\n",
       " 'a',\n",
       " 'process',\n",
       " 'called',\n",
       " 'network',\n",
       " 'pruning',\n",
       " 'to',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'smaller',\n",
       " 'and',\n",
       " 'more',\n",
       " 'efficient',\n",
       " 'to',\n",
       " 'run',\n",
       " 'by',\n",
       " 'removing',\n",
       " 'the',\n",
       " 'neurons',\n",
       " 'that',\n",
       " 'do',\n",
       " 'no',\n",
       " 'contribute',\n",
       " 'directly',\n",
       " 'to',\n",
       " 'output',\n",
       " 'there',\n",
       " 'are',\n",
       " 'ways',\n",
       " 'of',\n",
       " 'training',\n",
       " 'that',\n",
       " 'can',\n",
       " 'reduce',\n",
       " 'the',\n",
       " 'complexity',\n",
       " 'of',\n",
       " 'training',\n",
       " 'by',\n",
       " 'huge',\n",
       " 'amounts',\n",
       " 'dally',\n",
       " 'said']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "words_2 = tokenizer.tokenize(text)\n",
    "words_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "Looking throught the list, you might see a lot of words that do not bring the real context to the sentece. That is why we will use stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['january',\n",
       " 'google',\n",
       " 'launched',\n",
       " 'new',\n",
       " 'service',\n",
       " 'called',\n",
       " 'cloud',\n",
       " 'automl',\n",
       " 'automate',\n",
       " 'tricky',\n",
       " 'aspects',\n",
       " 'designing',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'software',\n",
       " 'working',\n",
       " 'project',\n",
       " 'company',\n",
       " 'researchers',\n",
       " 'sometimes',\n",
       " 'needed',\n",
       " 'run',\n",
       " 'many',\n",
       " '800',\n",
       " 'graphics',\n",
       " 'chips',\n",
       " 'unison',\n",
       " 'train',\n",
       " 'powerful',\n",
       " 'algorithms',\n",
       " 'unlike',\n",
       " 'humans',\n",
       " 'recognize',\n",
       " 'coffee',\n",
       " 'cups',\n",
       " 'seeing',\n",
       " 'one',\n",
       " 'two',\n",
       " 'examples',\n",
       " 'ai',\n",
       " 'networks',\n",
       " 'based',\n",
       " 'simulated',\n",
       " 'neurons',\n",
       " 'need',\n",
       " 'see',\n",
       " 'tens',\n",
       " 'thousands',\n",
       " 'examples',\n",
       " 'order',\n",
       " 'identify',\n",
       " 'object',\n",
       " 'imagine',\n",
       " 'trying',\n",
       " 'learn',\n",
       " 'recognize',\n",
       " 'every',\n",
       " 'item',\n",
       " 'environment',\n",
       " 'way',\n",
       " 'begin',\n",
       " 'understand',\n",
       " 'ai',\n",
       " 'software',\n",
       " 'requires',\n",
       " 'much',\n",
       " 'computing',\n",
       " 'power',\n",
       " 'researchers',\n",
       " 'could',\n",
       " 'design',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'could',\n",
       " 'trained',\n",
       " 'certain',\n",
       " 'tasks',\n",
       " 'using',\n",
       " 'handful',\n",
       " 'examples',\n",
       " 'would',\n",
       " 'upend',\n",
       " 'whole',\n",
       " 'paradigm',\n",
       " 'charles',\n",
       " 'bergan',\n",
       " 'vice',\n",
       " 'president',\n",
       " 'engineering',\n",
       " 'qualcomm',\n",
       " 'told',\n",
       " 'crowd',\n",
       " 'mit',\n",
       " 'technology',\n",
       " 'review',\n",
       " 'emtech',\n",
       " 'china',\n",
       " 'conference',\n",
       " 'earlier',\n",
       " 'week',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'become',\n",
       " 'capable',\n",
       " 'one',\n",
       " 'shot',\n",
       " 'learning',\n",
       " 'bergan',\n",
       " 'said',\n",
       " 'cumbersome',\n",
       " 'process',\n",
       " 'feeding',\n",
       " 'reams',\n",
       " 'data',\n",
       " 'algorithms',\n",
       " 'train',\n",
       " 'would',\n",
       " 'rendered',\n",
       " 'obsolete',\n",
       " 'could',\n",
       " 'serious',\n",
       " 'consequences',\n",
       " 'hardware',\n",
       " 'industry',\n",
       " 'existing',\n",
       " 'tech',\n",
       " 'giants',\n",
       " 'startups',\n",
       " 'currently',\n",
       " 'focused',\n",
       " 'developing',\n",
       " 'powerful',\n",
       " 'processors',\n",
       " 'designed',\n",
       " 'run',\n",
       " 'today',\n",
       " 'data',\n",
       " 'intensive',\n",
       " 'ai',\n",
       " 'algorithms',\n",
       " 'would',\n",
       " 'also',\n",
       " 'mean',\n",
       " 'vastly',\n",
       " 'efficient',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'trained',\n",
       " 'using',\n",
       " 'small',\n",
       " 'data',\n",
       " 'sets',\n",
       " 'reality',\n",
       " 'yet',\n",
       " 'research',\n",
       " 'already',\n",
       " 'done',\n",
       " 'making',\n",
       " 'algorithms',\n",
       " 'smaller',\n",
       " 'without',\n",
       " 'losing',\n",
       " 'accuracy',\n",
       " 'bill',\n",
       " 'dally',\n",
       " 'chief',\n",
       " 'scientist',\n",
       " 'nvidia',\n",
       " 'said',\n",
       " 'conference',\n",
       " 'nvidia',\n",
       " 'researchers',\n",
       " 'use',\n",
       " 'process',\n",
       " 'called',\n",
       " 'network',\n",
       " 'pruning',\n",
       " 'make',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'smaller',\n",
       " 'efficient',\n",
       " 'run',\n",
       " 'removing',\n",
       " 'neurons',\n",
       " 'contribute',\n",
       " 'directly',\n",
       " 'output',\n",
       " 'ways',\n",
       " 'training',\n",
       " 'reduce',\n",
       " 'complexity',\n",
       " 'training',\n",
       " 'huge',\n",
       " 'amounts',\n",
       " 'dally',\n",
       " 'said']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence = [w for w in words_2 if not w in stop_words]\n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Stemming is used to normalize the words. We have a lot of variations of the same word, carrying the same meaning other than when tense is involved. We stem to shorten up the lookup and normalize sentences. Keep in mind that this method can often create non-existent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['januari',\n",
       " 'googl',\n",
       " 'launch',\n",
       " 'new',\n",
       " 'servic',\n",
       " 'call',\n",
       " 'cloud',\n",
       " 'automl',\n",
       " 'autom',\n",
       " 'tricki',\n",
       " 'aspect',\n",
       " 'design',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'softwar',\n",
       " 'work',\n",
       " 'project',\n",
       " 'compani',\n",
       " 'research',\n",
       " 'sometim',\n",
       " 'need',\n",
       " 'run',\n",
       " 'mani',\n",
       " '800',\n",
       " 'graphic',\n",
       " 'chip',\n",
       " 'unison',\n",
       " 'train',\n",
       " 'power',\n",
       " 'algorithm',\n",
       " 'unlik',\n",
       " 'human',\n",
       " 'recogn',\n",
       " 'coffe',\n",
       " 'cup',\n",
       " 'see',\n",
       " 'one',\n",
       " 'two',\n",
       " 'exampl',\n",
       " 'ai',\n",
       " 'network',\n",
       " 'base',\n",
       " 'simul',\n",
       " 'neuron',\n",
       " 'need',\n",
       " 'see',\n",
       " 'ten',\n",
       " 'thousand',\n",
       " 'exampl',\n",
       " 'order',\n",
       " 'identifi',\n",
       " 'object',\n",
       " 'imagin',\n",
       " 'tri',\n",
       " 'learn',\n",
       " 'recogn',\n",
       " 'everi',\n",
       " 'item',\n",
       " 'environ',\n",
       " 'way',\n",
       " 'begin',\n",
       " 'understand',\n",
       " 'ai',\n",
       " 'softwar',\n",
       " 'requir',\n",
       " 'much',\n",
       " 'comput',\n",
       " 'power',\n",
       " 'research',\n",
       " 'could',\n",
       " 'design',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'could',\n",
       " 'train',\n",
       " 'certain',\n",
       " 'task',\n",
       " 'use',\n",
       " 'hand',\n",
       " 'exampl',\n",
       " 'would',\n",
       " 'upend',\n",
       " 'whole',\n",
       " 'paradigm',\n",
       " 'charl',\n",
       " 'bergan',\n",
       " 'vice',\n",
       " 'presid',\n",
       " 'engin',\n",
       " 'qualcomm',\n",
       " 'told',\n",
       " 'crowd',\n",
       " 'mit',\n",
       " 'technolog',\n",
       " 'review',\n",
       " 'emtech',\n",
       " 'china',\n",
       " 'confer',\n",
       " 'earlier',\n",
       " 'week',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'becom',\n",
       " 'capabl',\n",
       " 'one',\n",
       " 'shot',\n",
       " 'learn',\n",
       " 'bergan',\n",
       " 'said',\n",
       " 'cumbersom',\n",
       " 'process',\n",
       " 'feed',\n",
       " 'ream',\n",
       " 'data',\n",
       " 'algorithm',\n",
       " 'train',\n",
       " 'would',\n",
       " 'render',\n",
       " 'obsolet',\n",
       " 'could',\n",
       " 'seriou',\n",
       " 'consequ',\n",
       " 'hardwar',\n",
       " 'industri',\n",
       " 'exist',\n",
       " 'tech',\n",
       " 'giant',\n",
       " 'startup',\n",
       " 'current',\n",
       " 'focus',\n",
       " 'develop',\n",
       " 'power',\n",
       " 'processor',\n",
       " 'design',\n",
       " 'run',\n",
       " 'today',\n",
       " 'data',\n",
       " 'intens',\n",
       " 'ai',\n",
       " 'algorithm',\n",
       " 'would',\n",
       " 'also',\n",
       " 'mean',\n",
       " 'vastli',\n",
       " 'effici',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'train',\n",
       " 'use',\n",
       " 'small',\n",
       " 'data',\n",
       " 'set',\n",
       " 'realiti',\n",
       " 'yet',\n",
       " 'research',\n",
       " 'alreadi',\n",
       " 'done',\n",
       " 'make',\n",
       " 'algorithm',\n",
       " 'smaller',\n",
       " 'without',\n",
       " 'lose',\n",
       " 'accuraci',\n",
       " 'bill',\n",
       " 'dalli',\n",
       " 'chief',\n",
       " 'scientist',\n",
       " 'nvidia',\n",
       " 'said',\n",
       " 'confer',\n",
       " 'nvidia',\n",
       " 'research',\n",
       " 'use',\n",
       " 'process',\n",
       " 'call',\n",
       " 'network',\n",
       " 'prune',\n",
       " 'make',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'smaller',\n",
       " 'effici',\n",
       " 'run',\n",
       " 'remov',\n",
       " 'neuron',\n",
       " 'contribut',\n",
       " 'directli',\n",
       " 'output',\n",
       " 'way',\n",
       " 'train',\n",
       " 'reduc',\n",
       " 'complex',\n",
       " 'train',\n",
       " 'huge',\n",
       " 'amount',\n",
       " 'dalli',\n",
       " 'said']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmered_words = [ps.stem(w) for w in words_2 if not w in stop_words]\n",
    "stemmered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling words\n",
    "This is one of the most powerfull parts if NLTK module. It can label words in a sentence as nouns, adjectives, verbs...etc.\n",
    "When we had tokenized and stemmed the text now lets build function that will tag the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[('januari', 'NN')]\n",
      "[('googl', 'NN')]\n",
      "[('launch', 'NN')]\n",
      "[('new', 'JJ')]\n",
      "[('servic', 'NN')]\n",
      "[('call', 'NN')]\n",
      "[('cloud', 'NN')]\n",
      "[('automl', 'NN')]\n",
      "[('autom', 'NN')]\n",
      "[('tricki', 'NN')]\n",
      "[('aspect', 'NN')]\n",
      "[('design', 'NN')]\n",
      "[('machin', 'NN')]\n",
      "[('learn', 'NN')]\n",
      "[('softwar', 'NN')]\n",
      "[('work', 'NN')]\n",
      "[('project', 'NN')]\n",
      "[('compani', 'NN')]\n",
      "[('research', 'NN')]\n",
      "[('sometim', 'NN')]\n",
      "[('need', 'NN')]\n",
      "[('run', 'VB')]\n",
      "[('mani', 'NN')]\n",
      "[('800', 'CD')]\n",
      "[('graphic', 'JJ')]\n",
      "[('chip', 'NN')]\n",
      "[('unison', 'NN')]\n",
      "[('train', 'NN')]\n",
      "[('power', 'NN')]\n",
      "[('algorithm', 'NN')]\n",
      "[('unlik', 'NN')]\n",
      "[('human', 'NN')]\n",
      "[('recogn', 'NN')]\n",
      "[('coffe', 'NN')]\n",
      "[('cup', 'NN')]\n",
      "[('see', 'VB')]\n",
      "[('one', 'CD')]\n",
      "[('two', 'CD')]\n",
      "[('exampl', 'NN')]\n",
      "[('ai', 'NN')]\n",
      "[('network', 'NN')]\n",
      "[('base', 'NN')]\n",
      "[('simul', 'NN')]\n",
      "[('neuron', 'NN')]\n",
      "[('need', 'NN')]\n",
      "[('see', 'VB')]\n",
      "[('ten', 'NNS')]\n",
      "[('thousand', 'NN')]\n",
      "[('exampl', 'NN')]\n",
      "[('order', 'NN')]\n",
      "[('identifi', 'NN')]\n",
      "[('object', 'NN')]\n",
      "[('imagin', 'NN')]\n",
      "[('tri', 'NN')]\n",
      "[('learn', 'NN')]\n",
      "[('recogn', 'NN')]\n",
      "[('everi', 'NN')]\n",
      "[('item', 'NN')]\n",
      "[('environ', 'NN')]\n",
      "[('way', 'NN')]\n",
      "[('begin', 'NN')]\n",
      "[('understand', 'NN')]\n",
      "[('ai', 'NN')]\n",
      "[('softwar', 'NN')]\n",
      "[('requir', 'NN')]\n",
      "[('much', 'JJ')]\n",
      "[('comput', 'NN')]\n",
      "[('power', 'NN')]\n",
      "[('research', 'NN')]\n",
      "[('could', 'MD')]\n",
      "[('design', 'NN')]\n",
      "[('neural', 'JJ')]\n",
      "[('network', 'NN')]\n",
      "[('could', 'MD')]\n",
      "[('train', 'NN')]\n",
      "[('certain', 'JJ')]\n",
      "[('task', 'NN')]\n",
      "[('use', 'NN')]\n",
      "[('hand', 'NN')]\n",
      "[('exampl', 'NN')]\n",
      "[('would', 'MD')]\n",
      "[('upend', 'NN')]\n",
      "[('whole', 'JJ')]\n",
      "[('paradigm', 'NN')]\n",
      "[('charl', 'NN')]\n",
      "[('bergan', 'NN')]\n",
      "[('vice', 'NN')]\n",
      "[('presid', 'NN')]\n",
      "[('engin', 'NN')]\n",
      "[('qualcomm', 'NN')]\n",
      "[('told', 'NN')]\n",
      "[('crowd', 'NN')]\n",
      "[('mit', 'NN')]\n",
      "[('technolog', 'NN')]\n",
      "[('review', 'NN')]\n",
      "[('emtech', 'NN')]\n",
      "[('china', 'NN')]\n",
      "[('confer', 'NN')]\n",
      "[('earlier', 'RBR')]\n",
      "[('week', 'NN')]\n",
      "[('neural', 'JJ')]\n",
      "[('network', 'NN')]\n",
      "[('becom', 'NN')]\n",
      "[('capabl', 'NN')]\n",
      "[('one', 'CD')]\n",
      "[('shot', 'NN')]\n",
      "[('learn', 'NN')]\n",
      "[('bergan', 'NN')]\n",
      "[('said', 'VBD')]\n",
      "[('cumbersom', 'NN')]\n",
      "[('process', 'NN')]\n",
      "[('feed', 'NN')]\n",
      "[('ream', 'NN')]\n",
      "[('data', 'NNS')]\n",
      "[('algorithm', 'NN')]\n",
      "[('train', 'NN')]\n",
      "[('would', 'MD')]\n",
      "[('render', 'NN')]\n",
      "[('obsolet', 'NN')]\n",
      "[('could', 'MD')]\n",
      "[('seriou', 'NN')]\n",
      "[('consequ', 'NN')]\n",
      "[('hardwar', 'NN')]\n",
      "[('industri', 'NN')]\n",
      "[('exist', 'NN')]\n",
      "[('tech', 'NN')]\n",
      "[('giant', 'NN')]\n",
      "[('startup', 'NN')]\n",
      "[('current', 'JJ')]\n",
      "[('focus', 'NN')]\n",
      "[('develop', 'VB')]\n",
      "[('power', 'NN')]\n",
      "[('processor', 'NN')]\n",
      "[('design', 'NN')]\n",
      "[('run', 'VB')]\n",
      "[('today', 'NN')]\n",
      "[('data', 'NNS')]\n",
      "[('intens', 'NNS')]\n",
      "[('ai', 'NN')]\n",
      "[('algorithm', 'NN')]\n",
      "[('would', 'MD')]\n",
      "[('also', 'RB')]\n",
      "[('mean', 'NN')]\n",
      "[('vastli', 'NN')]\n",
      "[('effici', 'NN')]\n",
      "[('machin', 'NN')]\n",
      "[('learn', 'NN')]\n",
      "[('neural', 'JJ')]\n",
      "[('network', 'NN')]\n",
      "[('train', 'NN')]\n",
      "[('use', 'NN')]\n",
      "[('small', 'JJ')]\n",
      "[('data', 'NNS')]\n",
      "[('set', 'NN')]\n",
      "[('realiti', 'NN')]\n",
      "[('yet', 'RB')]\n",
      "[('research', 'NN')]\n",
      "[('alreadi', 'NN')]\n",
      "[('done', 'VBN')]\n",
      "[('make', 'VB')]\n",
      "[('algorithm', 'NN')]\n",
      "[('smaller', 'JJR')]\n",
      "[('without', 'IN')]\n",
      "[('lose', 'VB')]\n",
      "[('accuraci', 'NN')]\n",
      "[('bill', 'NN')]\n",
      "[('dalli', 'NN')]\n",
      "[('chief', 'NN')]\n",
      "[('scientist', 'NN')]\n",
      "[('nvidia', 'NN')]\n",
      "[('said', 'VBD')]\n",
      "[('confer', 'NN')]\n",
      "[('nvidia', 'NN')]\n",
      "[('research', 'NN')]\n",
      "[('use', 'NN')]\n",
      "[('process', 'NN')]\n",
      "[('call', 'NN')]\n",
      "[('network', 'NN')]\n",
      "[('prune', 'NN')]\n",
      "[('make', 'VB')]\n",
      "[('neural', 'JJ')]\n",
      "[('network', 'NN')]\n",
      "[('smaller', 'JJR')]\n",
      "[('effici', 'NN')]\n",
      "[('run', 'VB')]\n",
      "[('remov', 'NN')]\n",
      "[('neuron', 'NN')]\n",
      "[('contribut', 'NN')]\n",
      "[('directli', 'NN')]\n",
      "[('output', 'NN')]\n",
      "[('way', 'NN')]\n",
      "[('train', 'NN')]\n",
      "[('reduc', 'NN')]\n",
      "[('complex', 'JJ')]\n",
      "[('train', 'NN')]\n",
      "[('huge', 'JJ')]\n",
      "[('amount', 'NN')]\n",
      "[('dalli', 'NN')]\n",
      "[('said', 'VBD')]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in stemmered_words:\n",
    "            words_3 = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words_3)\n",
    "            print(tagged)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "            \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S In/IN)\n"
     ]
    }
   ],
   "source": [
    "# Do not work\n",
    "def process_content_and_draw():\n",
    "    try:\n",
    "        for i in words_2:\n",
    "            words_3 = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words_3)\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            print(chunked)\n",
    "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "                print(subtree)\n",
    "\n",
    "            chunked.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content_and_draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove words that appear only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = defaultdict(int)\n",
    "\n",
    "for token in filtered_sentence:\n",
    "    frequency[token] += 1\n",
    "        \n",
    "\n",
    "texts = [token for token in filtered_sentence if frequency[token] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['called',\n",
      " 'machine',\n",
      " 'learning',\n",
      " 'software',\n",
      " 'researchers',\n",
      " 'run',\n",
      " 'train',\n",
      " 'powerful',\n",
      " 'algorithms',\n",
      " 'recognize',\n",
      " 'one',\n",
      " 'examples',\n",
      " 'ai',\n",
      " 'networks',\n",
      " 'neurons',\n",
      " 'examples',\n",
      " 'recognize',\n",
      " 'ai',\n",
      " 'software',\n",
      " 'researchers',\n",
      " 'could',\n",
      " 'neural',\n",
      " 'networks',\n",
      " 'could',\n",
      " 'trained',\n",
      " 'using',\n",
      " 'examples',\n",
      " 'would',\n",
      " 'bergan',\n",
      " 'conference',\n",
      " 'neural',\n",
      " 'networks',\n",
      " 'one',\n",
      " 'learning',\n",
      " 'bergan',\n",
      " 'said',\n",
      " 'process',\n",
      " 'data',\n",
      " 'algorithms',\n",
      " 'train',\n",
      " 'would',\n",
      " 'could',\n",
      " 'powerful',\n",
      " 'run',\n",
      " 'data',\n",
      " 'ai',\n",
      " 'algorithms',\n",
      " 'would',\n",
      " 'efficient',\n",
      " 'machine',\n",
      " 'learning',\n",
      " 'neural',\n",
      " 'networks',\n",
      " 'trained',\n",
      " 'using',\n",
      " 'data',\n",
      " 'algorithms',\n",
      " 'smaller',\n",
      " 'dally',\n",
      " 'nvidia',\n",
      " 'said',\n",
      " 'conference',\n",
      " 'nvidia',\n",
      " 'researchers',\n",
      " 'process',\n",
      " 'called',\n",
      " 'network',\n",
      " 'neural',\n",
      " 'network',\n",
      " 'smaller',\n",
      " 'efficient',\n",
      " 'run',\n",
      " 'neurons',\n",
      " 'training',\n",
      " 'training',\n",
      " 'dally',\n",
      " 'said']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary\n",
    "\n",
    "[Bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) also known as vector space model, in this model a text is represented as the bag (multiset) of its words, disregarding grammar and word order but keeping multiplicity. So we are converting documents to vectors, where each vector is pair question-answer in style of:\n",
    "```\n",
    "“How many times does the word system appear in the document? Once.”\n",
    "```\n",
    "It is usefull to represent questions by their IDs. The mapping between questions and IDs is called a dictionary. So we will assign to each of the tokens (words) unique integer ID. In the end we see 30 distinc words in the processed corpus. We also save the dict and we will see if the words occur in other document as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ai': 0,\n",
       " 'algorithms': 1,\n",
       " 'bergan': 2,\n",
       " 'called': 3,\n",
       " 'conference': 4,\n",
       " 'could': 5,\n",
       " 'dally': 6,\n",
       " 'data': 7,\n",
       " 'efficient': 8,\n",
       " 'examples': 9,\n",
       " 'learning': 10,\n",
       " 'machine': 11,\n",
       " 'network': 12,\n",
       " 'networks': 13,\n",
       " 'neural': 14,\n",
       " 'neurons': 15,\n",
       " 'nvidia': 16,\n",
       " 'one': 17,\n",
       " 'powerful': 18,\n",
       " 'process': 19,\n",
       " 'recognize': 20,\n",
       " 'researchers': 21,\n",
       " 'run': 22,\n",
       " 'said': 23,\n",
       " 'smaller': 24,\n",
       " 'software': 25,\n",
       " 'train': 26,\n",
       " 'trained': 27,\n",
       " 'training': 28,\n",
       " 'using': 29,\n",
       " 'would': 30}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary([texts])\n",
    "dictionary.save('/tmp/article_about_ai_dictionary.dict')\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('second_text.txt', 'r') as f:\n",
    "    new_document=f.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Google just made it a lot easier to build your very own custom AI system.A new service, called Cloud AutoML, uses several machine-learning tricks to automatically build and train a deep-learning algorithm that can recognize things in images.The technology is limited for now, but it could be the start of something big. Building and optimizing a deep neural network algorithm normally requires a detailed understanding of the underlying math and code, as well as extensive practice tweaking the parameters of algorithms to get things just right. The difficulty of developing AI systems has created a race to recruit talent, and it means that only big companies with deep pockets can usually afford to build their own bespoke AI algorithms.â€śWe need to scale AI out to more people,â€ť Fei-Fei Li, chief scientist at Google Cloud, said ahead of the launch today. Li estimates there are at most a few thousand people worldwide with the expertise needed to build the very best deep-learning models. â€śBut there are an estimated 21 million developers worldwide today,â€ť she says. â€śWe want to reach out to them all, and make AI accessible to these developers.â€ťCloud computing is one of the keys to making AI more accessible. Google, Amazon, Microsoft, and other companies are rushing to add machine-learning capabilities to their cloud platforms. Google Cloud already offers many such tools, but they use pretrained models. That limits what they can doâ€”for example, programmers will only be able to use the tools to recognize a limited range of objects or scenes that they have already been trained to recognize. A new generation of cloud-based machine-learning tools that can train themselves would make the technology far more versatile and easier to use.Joaquin Vanschoren, a professor at the Eindhoven Institute of Technology in the Netherlands who specializes in automated machine learning, says itâ€™s still a relatively new research topic, though interest in the area has been heating up lately. â€śIt is impressive that they can release this as a production service so quickly,â€ť he says.Vanschoren says automation can add a lot of computational cost, so Google must be throwing plenty of resources at the service. Thatâ€™s only likely to get worse as programmers attempt to design AI systems that move beyond simple image classification and attempt to tackle ever broader tasks.Google researchers have been testing the limits of automating AI for some time now. In 2016, one team showed that deep learning could itself be used to identify the best tweaks to a deep-learning system. Last year another group at the company used simulated natural selection to â€śevolveâ€ť an optimal network architecture. And more recently, two Google scientists used reinforcement learningâ€”a technique inspired by the way animals learn through positive feedbackâ€”to automatically improve a deep-learning system.Efforts in this area might ultimately feed into the grand effort to build more general and adaptable forms of artificial intelligence. But before the machines take over completely, you can at least try your hand developing your very own AI.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
